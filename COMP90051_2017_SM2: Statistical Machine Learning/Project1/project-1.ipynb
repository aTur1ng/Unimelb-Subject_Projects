{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Linear Regression, Basis Expansion and Regularisation\n",
    "\n",
    "## Statistical Machine Learning (COMP90051), Semester 2, 2017\n",
    "\n",
    "*Copyright the University of Melbourne, 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will be required to write Python code, and also to answer a few theoretical questions. Your code will be assessed on the basis of (i) whether it works, e.g., does not result in an execution error, and (ii) whether it solves the specified task. It is not important whether your implementation is efficient, as long as it finishes execution within a reasonable amount of time. Note that the worksheet below is a combination of text, pre-implemented code and placeholders where we expect you to add your code and answers. Please follow the instructions carefully, **write your code and give answers only where specifically asked**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Marking:** You can get up to 22 marks for Project 1 (*this project*), and up to 33 marks for Project 2. The sum of marks for the two projects is then capped to 50 marks\n",
    "\n",
    "**Due date:** 5 PM on Monday, the 4th of September 2017 (AEST)\n",
    "\n",
    "**Late submissions** will incur a 10% penalty per calendar day\n",
    "\n",
    "** Submission format:** You should use this IPython Notebook worksheet as a starting point, fill in your code and answers where required, and submit the completed worksheet (.ipynb file) via LMS.\n",
    "\n",
    "**Academic Misconduct:** Your submission should contain only your own work and ideas. Where asked to write code, you cannot re-use someone else's code, and should write your own implementation. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "1. Linear Regression Model **(7 marks)**\n",
    "  1. Analytic Solution\n",
    "  2. Coordinate Descent Theory\n",
    "  3. Coordinate Descent Implementation\n",
    "  \n",
    "2. Basis Expansion and Regularisation **(7 marks)**\n",
    "  1. Polynomial Basis Functions\n",
    "  2. Radial Basis Functions\n",
    "  3. Ridge Regression\n",
    "\n",
    "3. Regression on Real Data **(8 marks)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression Model\n",
    "\n",
    "We start with setting up working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load some data. In the first two parts of this assignment we will be using a 1D dataset, because it is easy to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_synth = np.array([\n",
    "        -0.05167289, 0.63982603, 0.98123938, 1.24355676, 0.73023443, 0.53273977,\n",
    "        0.20670524, -0.12854164, 0.12142414, -0.76777309, -1.4140911, -1.33854328,\n",
    "        -1.71551542, -0.58528115, -0.87024253, -1.03868776, -0.67282369, -0.49697185,\n",
    "        -0.56960248, -0.18382946, 1.386896, 0.65562566, 0.83638401, 0.94085044,\n",
    "        1.3127219, 1.37959603, 0.20099471, 0.99189309, -0.04368285, -0.45082514])\n",
    "y_synth = y_synth[:,np.newaxis]\n",
    "n = y_synth.shape[0]\n",
    "x_synth = np.linspace(0.05, 0.95, n)\n",
    "x_synth = x_synth[:,np.newaxis]\n",
    "plt.plot(x_synth, y_synth, '.')\n",
    "\n",
    "# add a column of ones as a dummy feature\n",
    "x_dummy = np.ones(x_synth.shape)\n",
    "X_synth = np.column_stack((x_dummy, x_synth))\n",
    "X_synth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here *x* is the input and *y* the output. Below you will learn models to predict *y* given *x*, using *X_synth* and *y_synth* for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Do not overwrite global variables *X_synth*, *x_synth* and *y_synth*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Analytic Solution\n",
    "In this section, we apply linear regression directly. We will be using a regression with an intercept term. Therefore, despite the data being 1D, there are going to be two parameters.\n",
    "\n",
    "Implement the analytic solution to linear regression in the following cell. This solution can be found in lecture slides. *Hint: use numpy.linalg.inv to compute matrix inverse.*\n",
    "\n",
    "**Note:** For this task, you **cannot use** *numpy.linalg.solve*, *numpy.linalg.lstsq* or similar functions that provide an off-the-shelf implementation of the least-squares fit. Other libraries outside of *numpy* and *scipy* are also off limits, here and elsewhere in the project.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# X - is a matrix with N instances in rows and M features in columns\n",
    "#     constant one column should already be included within X\n",
    "# y - is an Nx1 column vector with response values\n",
    "#\n",
    "# Output:\n",
    "# w_hat - is an Mx1 column vector with fitted weights\n",
    "def least_squares_fit(X, y):\n",
    "    ... your code here ...\n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use your *least_squares_fit* function to fit linear model to the synthetic dataset. Compute and print resulting weights and the corresponding sum of squared residuals (SSR). Plot the data overlaid by the linear fit.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use \"least_squares_fit\" function from above\n",
    "# compute and print resulting weights and the corresponding SSR\n",
    "# plot the data overlaid by the linear fit\n",
    "\n",
    "... your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best a straightforward linear model can do. You can verify that one cannot do better, by manually changing weights to different values (instead of fitting it automatically). You should not be able to achieve a smaller SSR. The non-linearity in the data can be addressed using basis expansion. But before moving on to this, we will try a different approach to solve linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Coordinate Descent Theory\n",
    "In the next two sections, you will derive and implement coordinate descent algorithm for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the training data contains $N$ instances and $M$ features so that $x_{ij}$ denotes the value of feature $j$ for instance $i$. Assume no bias term. Let $y_i$ denote the response value for training instance $i$. Recall that in linear regression, a model with weights $\\textbf{w}$ is trained via minimisation of SSR, where the SSR is defined as\n",
    "\n",
    "$$L( \\textbf{w} ) = \\sum_{i=1}^N  \\left( y_i -  \\sum_{j=1}^M w_j x_{ij} \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let current weight estimates be $w_1,\\ldots,w_M$. Consider weight $w_1$, and its coordinate descent update, i.e., the solution for the optimal $w_1$ value that minimises $L(\\mathbf{w})$ while all other parameters are fixed. \n",
    "\n",
    "Prove that this update is\n",
    "\n",
    "$$w_1^{new} = \\frac{1}{\\sum_{i=1}^N x_{i1}^2} \\sum_{i=1}^N x_{i1}\\left(y_i - \\sum_{j=2}^M w_j x_{ij}  \\right) $$\n",
    "\n",
    "This involves partial differentiation of the SSR objective with respect to $w_1$, and equating this with 0. Show at least two intermediate steps. Clearly define any additional symbols that you introduce. Use LaTeX math typesetting.\n",
    "\n",
    "*Hint: to learn about LaTeX typesetting, see how the above cell is formatted. You can copy and modify the equation above, i.e., ```$$w_1^{new} = \\frac{1}{\\sum_{i=1}^N x_{i1}^2} \\sum_{i=1}^N x_{i1}\\left(y_i - \\sum_{j=2}^M w_j x_{ij}  \\right) $$```.  There are also good tutorials online, e.g., from the [sharelatex](https://www.sharelatex.com/learn/Mathematical_expressions) webpage.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here...**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Coordinate Descent Implementation\n",
    "\n",
    "Now implement and run coordinate descent for linear regression.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# X - is a matrix with N instances in rows and 2 columns,\n",
    "#     one of which is the constant one column\n",
    "# y - is an Nx1 column vector with response values\n",
    "# j - is the index of the weight to be updated, j=0,1\n",
    "# w_curr - is an 2x1 column vector with the current estimate of weights\n",
    "#\n",
    "# Output:\n",
    "# w_j_new - is a scalar containing the updated value of the j-th component\n",
    "def coord_descent_iteration(X, y, j, w_curr):\n",
    "    ... your code here ...    \n",
    "    return w_j_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to test your coordinate descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coordinate descent\n",
    "n_epochs = 10\n",
    "n_features = X_synth.shape[1]\n",
    "\n",
    "# starting guess\n",
    "w_current = np.array([1, 1], dtype='float')\n",
    "w_current.shape = (2,1)\n",
    "\n",
    "# keep track of iterative improvement\n",
    "w_trace = np.zeros((2, n_epochs*n_features))\n",
    "trace_step = 0\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    for j in range(n_features):\n",
    "        w_j_new = coord_descent_iteration(X_synth, y_synth, j, w_current)\n",
    "        w_current[j] = w_j_new;\n",
    "        w_trace[:,trace_step] = w_current[:,0]\n",
    "        trace_step += 1\n",
    "\n",
    "# analytic solution\n",
    "w_hat = least_squares_fit(X_synth, y_synth)\n",
    "\n",
    "# plot results\n",
    "plt.plot(w_trace[0,:], w_trace[1,:], '.-')\n",
    "plt.plot(w_hat[0], w_hat[1], 'r*')\n",
    "plt.xlabel('w_0')\n",
    "plt.ylabel('w_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basis Expansion and Regularisation\n",
    "\n",
    "The dataset provided is not well modelled by a linear regression. We will now look at more advanced techniques for modelling the data using richer basis functions to represent the input, thus learning non-linear regression functions.\n",
    "\n",
    "### 2.1 Polynomial Basis Functions\n",
    "\n",
    "In this section, we explore polynomial basis functions. These basis functions are fairly straightforward for 1D data, as illustrated in the example snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot polynomial functions, and a value of example input point\n",
    "x_example = 0.5\n",
    "\n",
    "n_poly = 5\n",
    "\n",
    "x_poly = np.linspace(0.05, 0.95, 100)\n",
    "x_phi = np.empty((n_poly, 1))\n",
    "\n",
    "for i in range(n_poly):\n",
    "    y_poly = x_poly ** (i + 1)\n",
    "    v = x_example ** (i + 1)\n",
    "    x_phi[i] = v\n",
    "    plt.plot(x_poly, y_poly)\n",
    "    plt.plot(x_example, v, 'ko')\n",
    "    \n",
    "print('original example point x =', x_example)\n",
    "print('transformed example point phi(x) =', np.transpose(x_phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the example point is 1D in original feature space, and 5D in the new feature space.\n",
    "\n",
    "Your task is to implement a method to apply a $M^{th}$ order polynomial expansion to a 1D input vector.\n",
    "\n",
    "<br>\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# x - is an Nx1 column vector of features\n",
    "# M - is the maximum polynomial degree, the new basis\n",
    "#     should include M polynomials with degrees from 1 to M\n",
    "#\n",
    "# Output:\n",
    "# Phi - is a NxM matrix of data in transformed feature space\n",
    "#       dummy feature is not included\n",
    "def expand_1d_to_poly(x, M):\n",
    "    ... your code here ...\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply the transformation and use your *least_squares_fit* function to fit linear model in the transformed feature space. Do not forget to add the dummy variable. Compute and print resulting weights and the corresponding SSR. Plot original 1D data overlaid by the  fit.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use some maximum degree M > 2\n",
    "\n",
    "# use \"expand_1d_to_poly\" function from above\n",
    "# use \"least_squares_fit\" function from above\n",
    "\n",
    "# compute and print resulting weights and the corresponding SSR\n",
    "# plot the data overlaid by the fit\n",
    "\n",
    "... your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the fit is now non-linear in the original features space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Radial Basis Functions\n",
    "\n",
    "Another popular choice for basis expansion is radial basis functions (RBF). For our synthetic 1D data, we will use an RBF defined as $v(x)=\\exp \\left(-\\frac{1}{\\sigma}||x - z||^2\\right)$. Parameter $z$ controls the location of the function, and $\\sigma$ controls the spread. This function is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# x - is a column vector of input values\n",
    "# z - is a scalar that controls location\n",
    "# s - is a scalar that controls spread\n",
    "#\n",
    "# Output:\n",
    "# v - contains the values of RBF evaluated for each element x\n",
    "#     v has the same dimensionality as x\n",
    "def radial_basis_function(x, z, s):\n",
    "    # ensure that t is a column vector\n",
    "    x = np.array(x)\n",
    "    if x.size == 1:\n",
    "        x.shape = (1,1)\n",
    "    else:\n",
    "        x_length = x.shape[0]\n",
    "        x.shape = (x_length, 1)\n",
    "    \n",
    "    # compute RBF value\n",
    "    r = np.linalg.norm(x - z, 2, 1)\n",
    "    v = np.exp(-r**2/s)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the code below to plot some RBF functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of RBF\n",
    "n_rbf = 5\n",
    "\n",
    "# locations and scale\n",
    "z = np.linspace(0, 1, n_rbf)\n",
    "sigma = 0.01 # same scale for each RBF\n",
    "\n",
    "x_rbf = np.linspace(-1, 2, 100)\n",
    "x_rbf = np.transpose(x_rbf)\n",
    "\n",
    "for i in range(n_rbf):\n",
    "    y_rbf = radial_basis_function(x_rbf, z[i], sigma)\n",
    "    plt.plot(x_rbf, y_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an example point, visualise values of that point in the RBF space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_example = 0.5\n",
    "\n",
    "# number of RBF\n",
    "n_rbf = 5\n",
    "\n",
    "# locations and scale\n",
    "z = np.linspace(0, 1, n_rbf)\n",
    "sigma = 0.01 # same scale for each RBF\n",
    "\n",
    "x_rbf = np.linspace(-1, 2, 100)\n",
    "x_rbf = np.transpose(x_rbf)\n",
    "\n",
    "x_phi = np.zeros((n_rbf, 1))\n",
    "for i in range(n_rbf):\n",
    "    y_rbf = radial_basis_function(x_rbf, z[i], sigma)\n",
    "    v = radial_basis_function(x_example, z[i], sigma)\n",
    "    x_phi[i] = v\n",
    "    plt.plot(x_rbf, y_rbf)\n",
    "    plt.plot(x_example, v, 'ko')\n",
    "\n",
    "print('original example point x =', x_example)\n",
    "print('transformed example point phi(x) =', np.transpose(x_phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the example point is 1D in original feature space, and 5D in the new feature space.\n",
    "\n",
    "Now implement feature space transformation using RBFs.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# x - is an Nx1 column vector\n",
    "# z - is an Mx1 column vector with locations for each of M RBFs\n",
    "# s - is a scalar that controls spread, shared between all RBFs\n",
    "#\n",
    "# Output:\n",
    "# Phi - is an NxM matrix, such that Phi(i,j) is the \n",
    "#       RBF transformation of x(i) using location z(j) and scale s\n",
    "def expand_1d_to_RBF(x, z, s):\n",
    "    ... your code here ...\n",
    "    ... in your code use \"radial_basis_function\" from above ...\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply the RBF transformation and use your *least_squares_fit* function to fit linear model in the transformed feature space. Do not forget to add the dummy variable. Compute and print resulting weights and the corresponding SSR. Plot original 1D data overlaid by the  fit.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can use the same number of RBFs, their\n",
    "# locations and scale as in the previous plot\n",
    "\n",
    "# use \"expand_1d_to_RBF\" function from above\n",
    "# use \"least_squares_fit\" function from above\n",
    "\n",
    "# compute and print resulting weights and the corresponding SSR\n",
    "# plot the data overlaid by the fit\n",
    "\n",
    "# number of RBF\n",
    "n_rbf = 5\n",
    "\n",
    "# locations and scale\n",
    "z = np.linspace(0, 1, n_rbf)\n",
    "sigma = 0.1 # same scale for each RBF\n",
    "\n",
    "... your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Compare the three methods of regression on this dataset: plain linear approach, polynomial basis expansion, and RBF basis expansion. For each method, name at least one aspect in which this method is advantageous compared to the other two methods. Justify your answer.\n",
    "\n",
    "*Hint: think about making predictions outside the range of training data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Write your answer here (a short paragraph or two).**</font> (as a *markdown* cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Ridge Regression\n",
    "\n",
    "In this section, we are going to use ridge regression to achieve two goals: (i) resistance to irrelevant features and (ii) control over the model complexity.\n",
    "\n",
    "Our current solution cannot handle irrelevant features. For example, if matrix with data contains duplicate columns (features), the operation of taking the inverse will result in an execution error. Note that computing matrix inverse with nearly collinear (irrelevant) features will not result in an execution error, but might produce a nonsense result. Methods for detecting near collinearity are outside the scope of this project. Therefore, if you suspect (near) collinearity, use one of regularisation methods, e.g., ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement ridge regression. *Hint: use np.linalg.inv*\n",
    "\n",
    "**Note:** For this task, you **cannot use** *sklearn.linear_model.Ridge*, *numpy.linalg.lstsq* or similar functions that provide an off-the-shelf implementation of ridge regression.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Write your code in the cell below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# X - is a matrix with N instances in rows and M features in columns\n",
    "#     constant one column should already be included within X\n",
    "# y - is an Nx1 column vector with response values\n",
    "# l - is a scalar, regularisation parameter\n",
    "#\n",
    "# Output:\n",
    "# w_hat - is an Mx1 column vector with fitted weights\n",
    "def reg_least_squares_fit(X, y, l):\n",
    "    ... your code here ...\n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to use held-out validation in order to assess the ability of our model to *extrapolate*. We will control model complexity using your implementation of ridge regression implemented.\n",
    "\n",
    "Plot train and test error as a function of the regularisation parameter.\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Complete the code below ...**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# students to write this code\n",
    "\n",
    "# fill in the gaps\n",
    "# use \"expand_1d_to_RBF\" and \"reg_least_squares_fit\" functions from above\n",
    "# do not forget to add a dummy variable\n",
    "\n",
    "# number of RBF\n",
    "n_rbf = 10\n",
    "\n",
    "# locations and scale\n",
    "z = np.linspace(0, 1, n_rbf)\n",
    "sigma = 0.01 # same scale for each RBF\n",
    "\n",
    "# train and test split for the purpose of extrapolation\n",
    "np.random.seed(2017) # for reproducibility\n",
    "n_instances = x_synth.shape[0]\n",
    "idx_train = range(0, n_instances, 2)\n",
    "idx_test = range(1, n_instances, 2)\n",
    "x_synth_train = x_synth[idx_train,:]\n",
    "y_synth_train = y_synth[idx_train,:]\n",
    "x_synth_test = x_synth[idx_test,:]\n",
    "y_synth_test = y_synth[idx_test,:]\n",
    "\n",
    "# vary the regularisation parameter\n",
    "l_start = 0.02\n",
    "l_stop = 1\n",
    "l_step = 0.02\n",
    "\n",
    "l_values = []\n",
    "ssr_trend_train = []\n",
    "ssr_trend_test = []\n",
    "\n",
    "l = l_start\n",
    "while l <= l_stop:\n",
    "    ... your code here ...\n",
    "    l += l_step\n",
    "\n",
    "plt.plot(l_values, ssr_trend_train, '.-b')\n",
    "plt.plot(l_values, ssr_trend_test, '.-g')\n",
    "plt.xlabel('parameter l')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Regression on Real Data\n",
    "\n",
    "In the final part of this project, we will work with a real-world dataset. Here, we will use linear regression to predict petrol consumption in liters per 10 km from the following features: number of cylinders, displacement, horsepower, weight, acceleration, model year. All of these features are different characteristics of cars. The exact meaning of each quantity is not important for the task. Run the following cell to load the data into variables *X_real* (cars in rows and features in columns) and *y_real* (response values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_real = np.array([\n",
    "[6.0, 250.0, 72.0, 3432.0, 21.0, 75.0],\n",
    "[6.0, 250.0, 100.0, 3282.0, 15.0, 71.0],\n",
    "[6.0, 231.0, 110.0, 3415.0, 15.8, 81.0],\n",
    "[4.0, 140.0, 75.0, 2542.0, 17.0, 74.0],\n",
    "[8.0, 351.0, 158.0, 4363.0, 13.0, 73.0],\n",
    "[4.0, 68.0, 49.0, 1867.0, 19.5, 73.0],\n",
    "[8.0, 318.0, 150.0, 4190.0, 13.0, 76.0],\n",
    "[8.0, 318.0, 140.0, 3735.0, 13.2, 78.0],\n",
    "[6.0, 156.0, 122.0, 2807.0, 13.5, 73.0],\n",
    "[4.0, 97.0, 60.0, 1834.0, 19.0, 71.0],\n",
    "[8.0, 302.0, 139.0, 3570.0, 12.8, 78.0],\n",
    "[6.0, 232.0, 100.0, 2789.0, 15.0, 73.0],\n",
    "[4.0, 91.0, 53.0, 1795.0, 17.5, 75.0],\n",
    "[6.0, 198.0, 95.0, 2833.0, 15.5, 70.0],\n",
    "[4.0, 97.0, 78.0, 2300.0, 14.5, 74.0],\n",
    "[8.0, 318.0, 210.0, 4382.0, 13.5, 70.0],\n",
    "[6.0, 198.0, 95.0, 3102.0, 16.5, 74.0],\n",
    "[8.0, 454.0, 220.0, 4354.0, 9.0, 70.0],\n",
    "[6.0, 232.0, 100.0, 2634.0, 13.0, 71.0],\n",
    "[4.0, 85.0, 70.0, 2070.0, 18.6, 78.0],\n",
    "[8.0, 318.0, 150.0, 3436.0, 11.0, 70.0],\n",
    "[4.0, 97.0, 88.0, 2279.0, 19.0, 73.0],\n",
    "[4.0, 121.0, 110.0, 2660.0, 14.0, 73.0],\n",
    "[6.0, 225.0, 85.0, 3465.0, 16.6, 81.0],\n",
    "[4.0, 105.0, 63.0, 2125.0, 14.7, 82.0],\n",
    "[8.0, 400.0, 150.0, 4997.0, 14.0, 73.0],\n",
    "[8.0, 318.0, 140.0, 4080.0, 13.7, 78.0],\n",
    "[4.0, 112.0, 85.0, 2575.0, 16.2, 82.0],\n",
    "[4.0, 98.0, 68.0, 2135.0, 16.6, 78.0],\n",
    "[6.0, 225.0, 100.0, 3651.0, 17.7, 76.0],\n",
    "[8.0, 340.0, 160.0, 3609.0, 8.0, 70.0],\n",
    "[8.0, 400.0, 167.0, 4906.0, 12.5, 73.0],\n",
    "[6.0, 200.0, 85.0, 2990.0, 18.2, 79.0],\n",
    "[4.0, 140.0, 90.0, 2264.0, 15.5, 71.0],\n",
    "[4.0, 151.0, 90.0, 2670.0, 16.0, 79.0],\n",
    "[8.0, 350.0, 145.0, 4055.0, 12.0, 76.0],\n",
    "[6.0, 258.0, 110.0, 3730.0, 19.0, 75.0],\n",
    "[6.0, 231.0, 115.0, 3245.0, 15.4, 79.0],\n",
    "[4.0, 91.0, 68.0, 2025.0, 18.2, 82.0],\n",
    "[6.0, 231.0, 105.0, 3380.0, 15.8, 78.0],\n",
    "[6.0, 168.0, 116.0, 2900.0, 12.6, 81.0],\n",
    "[8.0, 318.0, 150.0, 3399.0, 11.0, 73.0],\n",
    "[8.0, 400.0, 175.0, 5140.0, 12.0, 71.0],\n",
    "[8.0, 429.0, 198.0, 4952.0, 11.5, 73.0],\n",
    "[8.0, 302.0, 140.0, 4294.0, 16.0, 72.0],\n",
    "[8.0, 302.0, 140.0, 4638.0, 16.0, 74.0],\n",
    "[8.0, 455.0, 225.0, 4425.0, 10.0, 70.0],\n",
    "[4.0, 140.0, 90.0, 2408.0, 19.5, 72.0],\n",
    "[4.0, 120.0, 88.0, 2957.0, 17.0, 75.0],\n",
    "[6.0, 171.0, 97.0, 2984.0, 14.5, 75.0],\n",
    "])\n",
    "\n",
    "y_real = np.array([63.8, 80.8, 95.2, 106.3, 55.3, 123.3, 68.0, 82.5, 85.0, 114.8, 85.9, 76.5, 140.3, 93.5, 110.5, 46.8, 85.0, 59.5, 80.8, 167.5, 76.5, 85.0, 102.0, 74.8, 161.6, 46.8, 74.4, 131.8, 125.4, 85.0, 59.5, 51.0, 84.2, 119.0, 120.7, 55.3, 63.8, 91.4, 157.3, 87.6, 108.0, 63.8, 55.3, 51.0, 55.3, 59.5, 59.5, 85.0, 97.8, 76.5, ])\n",
    "y_real.shape = (X_real.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each step below, where answer is required, write no more than a few sentences. You can interleave code cells with markdown cells containing your answers. *Make it clear which of the questions 3.1 ... 3.3 each of your cell answers.*\n",
    "\n",
    "**3.1** Do you think that any preprocessing is required for this dataset? Explain your answer.\n",
    "\n",
    "**3.2** Fit ridge regression without any basis expansion. Set regularisation parameter to 0.01. Do you think a plain linear model is adequate for this dataset? Justify your answer, e.g., using a plot that characterises resulting fit.\n",
    "\n",
    "**3.3** Fit ridge regression in transformed feature space. You can use any feature transformation (polynomial, RBF or anything else). Justify your choice of the regularisation parameter. Compare results from steps 3.2 and 3.3. Do you think your results are reasonably good? Justify your answer. *Hint: think about prediction error per 10 liters*\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color='red'>**Write your code and answers below ...**</font> (use both code cells and markdown cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
